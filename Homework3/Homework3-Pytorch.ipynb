{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm going to do this in Pytorch \n",
    "\n",
    "TF is refusing to recongise cuda, but pytorch is.\n",
    "\n",
    "Going to have to use it so that this model can be trained in a resonable time. \n",
    "\n",
    "There is some issue with the configuration on my computer, and these models are too big to be worked on in colab without colab timing out. \n",
    "\n",
    "Pytorch is the standard when it comes to research, so I use it unapologetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jarvis/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Beautiful Import errors from TF\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Because every project starts here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our network\n",
    "\n",
    "class VGG(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super(VGG, self).__init__()\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    \n",
    "    # our pretrained model\n",
    "    model = models.vgg19(pretrained = True)\n",
    "    self.features = model.features\n",
    "\n",
    "    self.fc1 = nn.Linear(51200, 512)\n",
    "    self.d1 = nn.Dropout(.2)\n",
    "    self.fc2 = nn.Linear(512, 64)\n",
    "    self.d2 = nn.Dropout(.4)\n",
    "    self.f3 = nn.Linear(64, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get features flatten1\n",
    "    x = self.features(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.d1(x)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.d2(x)\n",
    "    return F.softmax(self.f3(x))\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = VGG(6)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BMP2', 'Background', 'Buk-M1-2', 'T14', 'T90', 'ZSU23']\n",
      "['BMP2', 'Background', 'Buk-M1-2', 'T14', 'T90', 'ZSU23']\n"
     ]
    }
   ],
   "source": [
    "#this is to get our data\n",
    "\n",
    "def load_train():\n",
    "    data_path = 'train'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # to prove that I got rid of the civilian cars\n",
    "    print(train_dataset.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "def load_val():\n",
    "    data_path = 'test'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # to prove that I got rid of the civilian cars\n",
    "    print(train_dataset.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "train = load_train()\n",
    "val = load_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch\t | Train Loss\t| Train Acc\t| Valid Loss\t| Valid Acc\t| Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "| 1\t | 1.8747\t| 0.9894\t| 2.0436\t| 0.9778\t| 1752.5857\n",
      "---------------------------------------------------------------------------\n",
      "| 2\t | 1.8749\t| 0.9528\t| 2.0436\t| 0.0889\t| 1759.3044\n",
      "---------------------------------------------------------------------------\n",
      "| 3\t | 1.8765\t| 0.9821\t| 2.0436\t| 0.9111\t| 1765.4253\n",
      "---------------------------------------------------------------------------\n",
      "| 4\t | 1.8767\t| 1.0154\t| 1.9186\t| 0.3556\t| 1767.5522\n",
      "---------------------------------------------------------------------------\n",
      "| 5\t | 1.8741\t| 1.0200\t| 1.9186\t| 1.0667\t| 1769.1581\n",
      "---------------------------------------------------------------------------\n",
      "| 6\t | 1.8741\t| 0.9804\t| 1.7936\t| 0.2667\t| 1764.5126\n",
      "---------------------------------------------------------------------------\n",
      "| 7\t | 1.8756\t| 0.9796\t| 1.9186\t| 1.1556\t| 1766.5298\n",
      "---------------------------------------------------------------------------\n",
      "| 8\t | 1.8739\t| 1.0038\t| 1.9186\t| 0.7333\t| 1767.7557\n",
      "---------------------------------------------------------------------------\n",
      "| 9\t | 1.8738\t| 0.9438\t| 1.7936\t| 1.2444\t| 1766.5541\n",
      "---------------------------------------------------------------------------\n",
      "| 10\t | 1.8747\t| 0.9870\t| 2.0436\t| 0.5333\t| 1767.7294\n",
      "---------------------------------------------------------------------------\n",
      "| 11\t | 1.8744\t| 1.0229\t| 2.0436\t| 1.4222\t| 1766.5021\n",
      "---------------------------------------------------------------------------\n",
      "| 12\t | 1.8747\t| 1.0070\t| 1.9186\t| 1.0889\t| 1770.2651\n",
      "---------------------------------------------------------------------------\n",
      "| 13\t | 1.8762\t| 1.0215\t| 1.9186\t| 0.5333\t| 1768.0930\n",
      "---------------------------------------------------------------------------\n",
      "| 14\t | 1.8747\t| 0.9905\t| 1.9186\t| 1.6000\t| 1770.8768\n",
      "---------------------------------------------------------------------------\n",
      "| 15\t | 1.8753\t| 1.0269\t| 1.7936\t| 1.0667\t| 1770.7721\n",
      "---------------------------------------------------------------------------\n",
      "| 16\t | 1.8753\t| 1.0101\t| 2.0436\t| 0.3778\t| 1773.3803\n",
      "---------------------------------------------------------------------------\n",
      "| 17\t | 1.8755\t| 1.0120\t| 1.7936\t| 0.8889\t| 1770.1943\n",
      "---------------------------------------------------------------------------\n",
      "| 18\t | 1.8755\t| 0.9429\t| 2.0436\t| 0.7111\t| 1773.7067\n",
      "---------------------------------------------------------------------------\n",
      "| 19\t | 1.8756\t| 1.0080\t| 2.0436\t| 0.1111\t| 1770.7725\n",
      "---------------------------------------------------------------------------\n",
      "| 20\t | 1.8750\t| 1.0360\t| 1.7936\t| 1.5333\t| 1782.0253\n",
      "---------------------------------------------------------------------------\n",
      "| 21\t | 1.8770\t| 1.0051\t| 1.9186\t| 0.6222\t| 1783.9436\n",
      "---------------------------------------------------------------------------\n",
      "| 22\t | 1.8756\t| 0.9810\t| 1.9186\t| 1.4222\t| 1781.1169\n",
      "---------------------------------------------------------------------------\n",
      "| 23\t | 1.8740\t| 1.0093\t| 1.9186\t| 0.5333\t| 1782.2277\n",
      "---------------------------------------------------------------------------\n",
      "| 24\t | 1.8744\t| 0.9867\t| 1.7936\t| 0.9778\t| 1781.7570\n",
      "---------------------------------------------------------------------------\n",
      "| 25\t | 1.8742\t| 0.9766\t| 2.0436\t| 1.7111\t| 1780.5650\n",
      "---------------------------------------------------------------------------\n",
      "| 26\t | 1.8736\t| 1.0396\t| 1.7936\t| 0.0889\t| 1782.9678\n",
      "---------------------------------------------------------------------------\n",
      "| 27\t | 1.8755\t| 0.9952\t| 1.7936\t| 1.3333\t| 1784.9026\n",
      "---------------------------------------------------------------------------\n",
      "| 28\t | 1.8758\t| 1.0347\t| 1.9186\t| 1.1556\t| 1785.6605\n",
      "---------------------------------------------------------------------------\n",
      "| 29\t | 1.8744\t| 0.9863\t| 2.0436\t| 1.6000\t| 1787.2519\n",
      "---------------------------------------------------------------------------\n",
      "| 30\t | 1.8751\t| 1.0227\t| 1.9186\t| 0.2000\t| 1785.5914\n",
      "\n",
      "Finished Training after 53199.68836069107\n"
     ]
    }
   ],
   "source": [
    "# we're going to save the best model for later\n",
    "best_acc = 0.0\n",
    "\n",
    "# history so we can GRAPH\n",
    "history = []\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# opti and crit\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)\n",
    "\n",
    "print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t| Time')\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        preds = torch.round(outputs)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels[0])\n",
    "        \n",
    "        # status bar\n",
    "        if i % 70 == 0:    \n",
    "            print(\"-\", end=\"\")\n",
    "            \n",
    "    train_loss = running_loss / (len(train.dataset))\n",
    "    train_acc = running_corrects / (len(train.dataset))\n",
    "    \n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(val):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # statistics\n",
    "        preds = torch.round(outputs)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels[0])\n",
    "        \n",
    "    val_loss = (running_loss )/ (len(val.dataset))\n",
    "    val_acc = running_corrects / (len(val.dataset))\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    \n",
    "    # save model if best\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = model.state_dict()\n",
    "    \n",
    "    # print out results so we can see in real time\n",
    "    print(\"\")\n",
    "    print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\".format(epoch + 1, train_loss, train_acc, val_loss, val_acc, epoch_time))\n",
    "    \n",
    "    # save our history\n",
    "    tmp = [train_loss, train_acc, val_loss, val_acc]\n",
    "    history.append(tmp)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f'Finished Training after {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "torch.save(best_model_wts, \"./BestVGGModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a standard CNN \n",
    "\n",
    "I will not be using VGG as a pre-trained model for feature extraction, will use bog-standard CNN with hopefully the same results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BMP2', 'Background', 'Buk-M1-2', 'T14', 'T90', 'ZSU23']\n",
      "['BMP2', 'Background', 'Buk-M1-2', 'T14', 'T90', 'ZSU23']\n"
     ]
    }
   ],
   "source": [
    "#this is to get our data\n",
    "\n",
    "def load_train():\n",
    "    data_path = 'train'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # to prove that I got rid of the civilian cars\n",
    "    print(train_dataset.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "def load_val():\n",
    "    data_path = 'test'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # to prove that I got rid of the civilian cars\n",
    "    print(train_dataset.classes)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "train = load_train()\n",
    "val = load_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoganNetwork(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super(BoganNetwork, self).__init__()\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "    self.d1 = nn.Dropout(.2)\n",
    "    self.pool = nn.AdaptiveAvgPool2d(5)\n",
    "    self.conv2 = nn.Conv2d(64, 32, 2)\n",
    "    self.d2 = nn.Dropout(.3)\n",
    "    self.fc1 = nn.Linear(512, 512)\n",
    "    self.d3 = nn.Dropout(.5)\n",
    "    self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get features flatten1\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = self.d1(x)\n",
    "    x = self.pool(x)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.d2(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.d3(x)\n",
    "    x = self.fc2(x)\n",
    "    return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model_new = BoganNetwork(6)\n",
    "model_new = model_new.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch\t | Train Loss\t| Train Acc\t| Valid Loss\t| Valid Acc\t| Time\n",
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "| 1\t | 1.5140\t| 1.0382\t| 1.4645\t| 0.4667\t| 219.0180\n",
      "------------------------------------------------------------------\n",
      "| 2\t | 1.3859\t| 0.9551\t| 1.2223\t| 1.4444\t| 219.0096\n",
      "------------------------------------------------------------------\n",
      "| 3\t | 1.3213\t| 0.9854\t| 1.2796\t| 0.1778\t| 218.9087\n",
      "------------------------------------------------------------------\n",
      "| 4\t | 1.2734\t| 1.0172\t| 1.1341\t| 1.2333\t| 218.9198\n",
      "------------------------------------------------------------------\n",
      "| 5\t | 1.2355\t| 0.9896\t| 1.2891\t| 0.7222\t| 218.8991\n",
      "------------------------------------------------------------------\n",
      "| 6\t | 1.2085\t| 0.9892\t| 1.1408\t| 1.4556\t| 218.6547\n",
      "------------------------------------------------------------------\n",
      "| 7\t | 1.1896\t| 0.9774\t| 1.1653\t| 0.1667\t| 218.6734\n",
      "------------------------------------------------------------------\n",
      "| 8\t | 1.1730\t| 1.0400\t| 1.0450\t| 1.0556\t| 218.4293\n",
      "------------------------------------------------------------------\n",
      "| 9\t | 1.1621\t| 1.0128\t| 1.1167\t| 1.0778\t| 218.7096\n",
      "------------------------------------------------------------------\n",
      "| 10\t | 1.1537\t| 1.0317\t| 1.2127\t| 1.9556\t| 218.0239\n",
      "------------------------------------------------------------------\n",
      "| 11\t | 1.1463\t| 0.9742\t| 1.2139\t| 1.0667\t| 218.0060\n",
      "------------------------------------------------------------------\n",
      "| 12\t | 1.1403\t| 0.9872\t| 1.0496\t| 0.1667\t| 217.7526\n",
      "------------------------------------------------------------------\n",
      "| 13\t | 1.1346\t| 0.9683\t| 1.1100\t| 0.8889\t| 217.9569\n",
      "------------------------------------------------------------------\n",
      "| 14\t | 1.1298\t| 0.9413\t| 1.1733\t| 0.7333\t| 217.6613\n",
      "------------------------------------------------------------------\n",
      "| 15\t | 1.1258\t| 0.9751\t| 1.0745\t| 1.7778\t| 217.5667\n",
      "------------------------------------------------------------------\n",
      "| 16\t | 1.1227\t| 0.9726\t| 1.1052\t| 0.2889\t| 217.5670\n",
      "------------------------------------------------------------------\n",
      "| 17\t | 1.1171\t| 0.9886\t| 1.0601\t| 0.1667\t| 217.3914\n",
      "------------------------------------------------------------------\n",
      "| 18\t | 1.1105\t| 1.0051\t| 1.1262\t| 0.8889\t| 217.5507\n",
      "------------------------------------------------------------------\n",
      "| 19\t | 1.1073\t| 0.9818\t| 1.0511\t| 0.0000\t| 217.3269\n",
      "------------------------------------------------------------------\n",
      "| 20\t | 1.1020\t| 1.0123\t| 1.1652\t| 1.2333\t| 217.3582\n",
      "------------------------------------------------------------------\n",
      "| 21\t | 1.0984\t| 1.0153\t| 1.1750\t| 0.8889\t| 217.1135\n",
      "------------------------------------------------------------------\n",
      "| 22\t | 1.0956\t| 1.0075\t| 1.0689\t| 0.0000\t| 217.3310\n",
      "------------------------------------------------------------------\n",
      "| 23\t | 1.0908\t| 1.0406\t| 1.0612\t| 1.2444\t| 217.2318\n",
      "------------------------------------------------------------------\n",
      "| 24\t | 1.0901\t| 0.9493\t| 1.1281\t| 2.3333\t| 217.1670\n",
      "------------------------------------------------------------------\n",
      "| 25\t | 1.0898\t| 1.0054\t| 1.0503\t| 0.8889\t| 217.2923\n",
      "------------------------------------------------------------------\n",
      "| 26\t | 1.0868\t| 1.0300\t| 1.1019\t| 1.6222\t| 217.3252\n",
      "------------------------------------------------------------------\n",
      "| 27\t | 1.0851\t| 1.0347\t| 1.0438\t| 0.8889\t| 217.0482\n",
      "------------------------------------------------------------------\n",
      "| 28\t | 1.0839\t| 1.0382\t| 1.0449\t| 0.0000\t| 217.1337\n",
      "------------------------------------------------------------------\n",
      "| 29\t | 1.0832\t| 1.0068\t| 1.1118\t| 1.0667\t| 217.2082\n",
      "------------------------------------------------------------------\n",
      "| 30\t | 1.0822\t| 0.9932\t| 1.0446\t| 0.3556\t| 217.1598\n",
      "\n",
      "Finished Training after 6535.402711868286\n"
     ]
    }
   ],
   "source": [
    "# we're going to save the best model for later\n",
    "best_acc = 0.0\n",
    "\n",
    "# history so we can GRAPH\n",
    "history = []\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# opti and crit\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_new.parameters(), lr = 0.0005)\n",
    "\n",
    "print('| Epoch\\t | Train Loss\\t| Train Acc\\t| Valid Loss\\t| Valid Acc\\t| Time')\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_new(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        preds = torch.round(outputs)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels[0])\n",
    "        \n",
    "        # status bar\n",
    "        if i % 40 == 0:    \n",
    "            print(\"-\", end=\"\")\n",
    "            \n",
    "    train_loss = running_loss / (len(train.dataset))\n",
    "    train_acc = running_corrects / (len(train.dataset))\n",
    "    \n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(val):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model_new(inputs)\n",
    "\n",
    "        # statistics\n",
    "        preds = torch.round(outputs)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels[0])\n",
    "        \n",
    "    val_loss = (running_loss )/ (len(val.dataset))\n",
    "    val_acc = running_corrects / (len(val.dataset))\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    \n",
    "    # save model if best\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = model_new.state_dict()\n",
    "    \n",
    "    # print out results so we can see in real time\n",
    "    print(\"\")\n",
    "    print(\"| {}\\t | {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\\t| {:.4f}\".format(epoch + 1, train_loss, train_acc, val_loss, val_acc, epoch_time))\n",
    "    \n",
    "    # save our history\n",
    "    tmp = [train_loss, train_acc, val_loss, val_acc]\n",
    "    history.append(tmp)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f'Finished Training after {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "torch.save(best_model_wts, \"./BestCNNModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conculsion\n",
    "\n",
    "Seems like the standard CNN model did better when comparing the best model it produced to the best model that the best VGG model, however I think that this can be a bit misleading. \n",
    "\n",
    "The Validation set was very small, and had data that was not as mixed (having both the images from the game and real life), so it was hard to see if a model truly generalised or simply just got lucky. With a larger training set, it should be more clear if one model is truly better than another in the sense of accuracy. Another change would be to have two validation sets, one with the synthetic data and one with the real-life data. This could tell us if the model was generalising in terms of the synthetic and/or real data, which would be useful to know for gauging the overeall performance of the model.\n",
    "\n",
    "Another issue was training time, and GPU memory limitations. I trained on a GTX 980ti graphics card with 6 gb of memory. I was having issues with my card not having enough memory to hold both the VGG model and a batch larger then 8. Those smaller batch sizes could have affected learning, making it not a fair apples to apples comparison to the standard CNN model. Being on a older card, The VGG model took a very long time to train in comparison to the standard CNN model. Luckly for me, my commputer turned out to be a very nice space heater for the recent cold front, keep me very warm these last few days :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
